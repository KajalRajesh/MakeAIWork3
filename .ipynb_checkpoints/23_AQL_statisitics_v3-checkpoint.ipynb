{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d2fdd76a-9269-48b8-9608-10043aaa97bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import van het model uit resnet_cuda4.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fafcbc54-abab-43f7-acd8-c69a3da38920",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\MakeAIWork3\\env\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Lenovo\\MakeAIWork3\\env\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "from torchvision.models import resnet18\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "num_classes = 4\n",
    "\n",
    "#Resnet model\n",
    "model = resnet18(pretrained=True)\n",
    "num_features = model.fc.in_features\n",
    "model.fc = nn.Linear(num_features, num_classes)\n",
    "model = model.to(device)\n",
    "\n",
    "# model = torch.load('Team_Complete_Model.pth')\n",
    "# model = model.to(device)\n",
    "# model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0ffb5ec2-8915-4777-8c78-63f063455774",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Labels: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1]\n",
      "Predicted Labels: [2, 2, 1, 2, 1, 2, 1, 0, 1, 0, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 1, 1, 3]\n",
      "True Labels: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "Predicted Labels: [3, 1, 2, 1, 2, 3, 2, 2, 2, 2, 3, 2, 3, 3, 2, 3, 2, 3, 1, 3, 2, 2, 2, 2, 3, 3, 2, 2, 2, 2, 2, 2]\n",
      "True Labels: [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3]\n",
      "Predicted Labels: [0, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 0, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "True Labels: [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]\n",
      "Predicted Labels: [2, 2, 3, 2, 2, 2, 1, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "104\n",
      "91.22807017543859\n",
      "Klasse 4: naar de varkens ermee!\n",
      "Batch Statistics:\n",
      "-----------------\n",
      "Total Images: 114\n",
      "Correctly Classified Images (Good): 38\n",
      "Percentage of Bad Images: 91.22807017543859%\n",
      "Average Loss: 1.4066269993782043\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "image_size = 224\n",
    "batch_size = 32\n",
    "\n",
    "test_dir = \"C:/Users/Lenovo/MakeAIWork3/images/Test1\"\n",
    "\n",
    "# Define the transformations for test data before entering the neural network\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.Resize((image_size, image_size)),                     # Resize images to 224x224 pixels\n",
    "    transforms.ToTensor(),                                           # Convert images to tensors\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])  # Normalize the images\n",
    "])\n",
    "\n",
    "# Load the test dataset\n",
    "test_dataset = ImageFolder(test_dir, transform=transform_test)\n",
    "\n",
    "# test_dataset.class_to_idx\n",
    "# idx2class = {v: k for k, v in test_dataset.class_to_idx.items()}\n",
    "# idx2class\n",
    "\n",
    "\n",
    "# Create DataLoaders for managing the data batches\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# def evaluate_acceptance_quality_batch(model, test_loader, device, threshold):\n",
    "#     model.eval()\n",
    "#     correct = 0\n",
    "#     total = 0\n",
    "#     bad_images = []\n",
    "#     criterion = nn.CrossEntropyLoss()\n",
    "#     running_loss = 0.0\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         for images, labels in test_loader:\n",
    "#             images = images.to(device)\n",
    "#             labels = labels.to(device)\n",
    "\n",
    "#             outputs = model(images)\n",
    "#             _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "#             # Check if the predicted label is \"good\" (class index 0)\n",
    "#             bad_images.extend([i for i, p in enumerate(predicted) if p != 1])\n",
    "\n",
    "#             total += labels.size(0)\n",
    "#             correct += (predicted == labels).sum().item()\n",
    "\n",
    "#             loss = criterion(outputs, labels)\n",
    "#             running_loss += loss.item()\n",
    "\n",
    "#             # Print the labels\n",
    "#             print(\"True Labels:\", labels.tolist())\n",
    "#             print(\"Predicted Labels:\", predicted.tolist())\n",
    "\n",
    "#     accuracy = 100.0 * correct / total\n",
    "#     print(len(bad_images))\n",
    "#     perc_bad_images = (len(bad_images) / total) * 100\n",
    "#     print(perc_bad_images)\n",
    "#     if perc_bad_images <= threshold:\n",
    "#         print(\"Klasse 1: naar supermarkt\")\n",
    "#     elif threshold < perc_bad_images <= 6.5:\n",
    "#         print(\"Klasse 2: naar appelmoesfabriek\")\n",
    "#     elif 6.6 < perc_bad_images <= 15:\n",
    "#         print(\"Klasse 3: naar appelstroopfabriek\")\n",
    "#     else:\n",
    "#         print(\"Klasse 4: naar de varkens ermee!\")\n",
    "\n",
    "#     print(\"Batch Statistics:\")\n",
    "#     print(\"-----------------\")\n",
    "#     print(f\"Total Images: {total}\")\n",
    "#     print(f\"Correctly Classified Images (Good): {correct}\")\n",
    "#     print(f\"Percentage of Bad Images: {perc_bad_images}%\")\n",
    "#     print(f\"Average Loss: {running_loss / len(test_loader)}\")\n",
    "# no = evaluate_acceptance_quality_batch(model, test_loader, device, 0.4)    \n",
    "# print(no)\n",
    "    # if perc_bad_images <= threshold:\n",
    "    #     print(\"Klasse 1: naar supermarkt\")\n",
    "    # elif threshold < perc_bad_images <= 6.5:\n",
    "    #     print(\"Klasse 2: naar appelmoesfabriek\")\n",
    "    # elif perc_bad_images >= 15:\n",
    "    #     print(\"Klasse 3: naar appelstroopfabriek\")\n",
    "    # else:\n",
    "    #     print(\"Klasse 4: naar de varkens ermee!\")\n",
    "\n",
    "    # print(\"Batch Statistics:\")\n",
    "    # print(\"-----------------\")\n",
    "    # print(f\"Total Images: {total}\")\n",
    "    # print(f\"Correctly Classified Images (Good): {correct}\")\n",
    "    # print(f\"Percentage of Bad Images: {perc_bad_images}%\")\n",
    "    # print(f\"Average Loss: {running_loss / len(test_loader)}\")\n",
    "# def test(model, criterion, test_loader, device):\n",
    "#     model.eval()\n",
    "#     correct = 0\n",
    "#     total = 0\n",
    "#     running_loss = 0.0\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         for inputs, labels in test_loader:\n",
    "#             inputs = inputs.to(device)\n",
    "#             labels = labels.to(device)\n",
    "\n",
    "#             outputs = model(inputs).to(device)\n",
    "#             loss = criterion(outputs, labels)\n",
    "\n",
    "#             _, predicted = torch.max(outputs.data, 1)\n",
    "#             total += labels.size(0)\n",
    "#             correct += (predicted == labels).sum().item()\n",
    "\n",
    "#             running_loss += loss.item()\n",
    "\n",
    "#     accuracy = 100.0 * correct / total\n",
    "#     return running_loss / len(test_loader), accuracy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9cbcc2c8-4e6f-4baf-afab-957cc3aaf534",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m torch.utils.collect_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3632f1e3-d775-4d52-bd36-246efed2db8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# test_loss = test(model, criterion, test_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2c1333d8-03b5-4ba4-8bd2-a61d79c91a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f'Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87209d31-2fb8-4a08-bb2d-f060eb6cdbbb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a96ac0d5-3d59-483d-bf42-e7c652c86b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "print(device)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "y_pred = []\n",
    "y_true = []\n",
    "\n",
    "# iterate over test data\n",
    "for inputs, labels in test_loader:\n",
    "    inputs = inputs.to(device)\n",
    "    labels = labels.to(device)\n",
    "    output = model(inputs) # Feed Network\n",
    "    output = (torch.max(torch.exp(output), 1)[1]).data.cpu().numpy()\n",
    "    # output=output.to(device)\n",
    "    y_pred.extend(output) # Save Prediction\n",
    "        \n",
    "    labels = labels.data.cpu().numpy()\n",
    "    # labels=labels.to(device)\n",
    "    y_true.extend(labels) # Save Truth\n",
    "\n",
    "\n",
    "# # constant for classes\n",
    "classes = ('0','1','2','3')\n",
    "\n",
    "# Build confusion matrix\n",
    "cf_matrix = confusion_matrix(y_true, y_pred)\n",
    "print(cf_matrix)\n",
    "\n",
    "confusion_matrix_df = pd.DataFrame(confusion_matrix(y_true, y_pred)).rename(columns=idx2class, index=idx2class)\n",
    "fig, ax = plt.subplots(figsize=(7,5))         \n",
    "sns.heatmap(confusion_matrix_df, annot=True, ax=ax);\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "787f54ff-49fe-46ca-95f2-60043ee2cb66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# maak op de assen onderscheid tussen y en yhat\n",
    "\n",
    "#68% prediction is good\n",
    "#32% prediction wrong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "14d113ef-31a6-4d19-8dd3-a62b7952921d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AQL berekening toevoegen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c7cb079e-86c4-444f-86fe-d848b8fa0079",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to show the images\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# def reverse_normalize(image):\n",
    "#     mean = [0.485, 0.456, 0.406]\n",
    "#     std = [0.229, 0.224, 0.225]\n",
    "#     image = image.clone()\n",
    "#     for i in range(3):\n",
    "#         image[i] = (image[i] * std[i]) + mean[i]\n",
    "#     return image\n",
    "\n",
    "# def show_batch(train_d, val_d):\n",
    "#     # Get a batch of data from the DataLoader\n",
    "#     data_train = next(iter(train_d))\n",
    "#     data_val = next(iter(val_d))\n",
    "\n",
    "#     # Set the savefig.bbox_inches parameter to 'tight'\n",
    "#     plt.rcParams[\"savefig.bbox_inches\"] = 'tight'\n",
    "\n",
    "#     # Retrieve the first tensor and its corresponding label\n",
    "#     image_train = data_train[0][0]\n",
    "#     image_val = data_val[0][0]\n",
    "#     label_train = data_train[1][0]\n",
    "#     label_val = data_val[1][0]\n",
    "\n",
    "#     # Reverse the normalization of the images\n",
    "#     image_train = reverse_normalize(image_train)\n",
    "#     image_val = reverse_normalize(image_val)\n",
    "\n",
    "#     # Convert the image tensors to NumPy arrays and transpose the dimensions\n",
    "#     np_image_train = image_train.permute(1, 2, 0).numpy()\n",
    "#     np_image_val = image_val.permute(1, 2, 0).numpy()\n",
    "\n",
    "#     # Create a figure with two subplots\n",
    "#     fig, axes = plt.subplots(1, 2)\n",
    "\n",
    "#     # Display the image in the first subplot\n",
    "#     axes[0].imshow(np_image_train)\n",
    "#     axes[0].set_title(f'{label_train}, {image_train.shape}')\n",
    "\n",
    "#     # Display the image in the second subplot\n",
    "#     axes[1].imshow(np_image_val)\n",
    "#     axes[1].set_title(f'{label_val}, {image_val.shape}')\n",
    "\n",
    "#     # Adjust spacing between subplots to prevent overlap\n",
    "#     fig.tight_layout()\n",
    "\n",
    "#     # Save the figure with a tight bounding box\n",
    "#     plt.savefig('figure.png', bbox_inches='tight')\n",
    "\n",
    "#     # Show the plot\n",
    "#     plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
